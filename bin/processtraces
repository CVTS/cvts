#!/usr/bin/env python3

import os
import json
import pickle
import tempfile
from collections import defaultdict
from multiprocessing import Pool
from cvts.settings import (
    RAW_PATH,
    CONFIG_PATH,
    MM_PATH,
    SEQ_PATH,
    VALHALLA_CONFIG_FILE)
from cvts import csvfiles2jsonchunks



POINT_KEYS = ('lat', 'lon', 'time', 'heading', 'speed', 'heading_tolerance')
TMP_DIR = tempfile.gettempdir()
EDGE_KEYS = ('way_id', 'speed', 'speed_limit')
EDGE_ATTR_NAMES = ('way_id', 'valhalla_speed', 'speed_limit')
NAS = ('NA',) * len(EDGE_KEYS)



def _getpointattrs(point):
    return tuple(point[k] for k in POINT_KEYS)

def _getedgeattrs(edge):
    return tuple(edge.get(k, 'NA') for k in EDGE_KEYS)

def _run_valhalla(fn, trip, trip_index):
    tmp_file_in  = os.path.join(TMP_DIR, str(trip_index) + '_in_'  + fn)
    tmp_file_out = os.path.join(TMP_DIR, str(trip_index) + '_out_' + fn)

    try:
        with open(tmp_file_in, 'w') as jf:
            json.dump(trip, jf)

        os.system('valhalla_service {} trace_attributes {} 2>/dev/null > {}'.format(
            VALHALLA_CONFIG_FILE,
            tmp_file_in,
            tmp_file_out))

        with open(tmp_file_out, 'r') as rfile:
            return json.load(rfile)

    finally:
        try: os.remove(tmp_file_in)
        except: pass
        try: os.remove(tmp_file_out)
        except: pass


def process_file(fns):
    fn = fns[0]
    input_files = fns[1]

    pnt_file = os.path.join(MM_PATH, fn)
    seq_file = os.path.join(SEQ_PATH, '{}.json'.format(os.path.splitext(fn)[0]))

    if os.path.exists(pnt_file) and os.path.exists(seq_file):
        return

    def run_trip(trip, trip_index):
        try:
            way_ids = {
                'trip_index': trip_index,
                'start': {'time': int(trip['shape'][ 0]['time']), 'loc': {'lat': trip['shape'][ 0]['lat'], 'lon': trip['shape'][ 0]['lon']}},
                'end':   {'time': int(trip['shape'][-1]['time']), 'loc': {'lat': trip['shape'][-1]['lat'], 'lon': trip['shape'][-1]['lon']}}}

            try:
                snapped = _run_valhalla(fn, trip, trip_index)
            except:
                raise Exception('valhalla failure')

            edges = snapped['edges']
            match_props = ((p.get('edge_index'), p['type']) for p in snapped['matched_points'])
            way_ids['status'] = 'success'
            way_ids['way_ids'] = [e['way_id'] for e in edges]
            return way_ids, [_getpointattrs(p) + \
                ('success', trip_index) + \
                (_getedgeattrs(edges[ei]) if ei is not None else NAS) + \
                (mt,) for p, (ei, mt) in zip(trip['shape'], match_props)]

        except Exception as e:
            e_str = str(e)
            way_ids['status'] = 'failure'
            way_ids['message'] = e_str
            return way_ids, [_getpointattrs(p) + \
                ('failure', trip_index) + \
                NAS + \
                (e_str,) for p in trip['shape']]

    try:
        with open(pnt_file, 'w') as resultsfile, open(seq_file , 'w') as seqfile:
            resultsfile.write(','.join(POINT_KEYS + ('status', 'trip_index') + EDGE_ATTR_NAMES + ('message',)) + '\n')
            def run_trips(way_ids, result):
                resultsfile.writelines('{}\n'.format(','.join(str(t) for t in tup)) for tup in result)
                return way_ids

            trips = csvfiles2jsonchunks(input_files, True)
            results = (run_trip(trip, ti) for ti, trip in enumerate(trips))
            json.dump([run_trips(*r) for r in results], seqfile)

    except Exception as e:
        print('{} failed: {}'.format(fn, e))

    else:
        print('{} passed'.format(fn))



# get all the input files
pickle_file_name = os.path.join(CONFIG_PATH, 'raw_files.pkl')
if os.path.exists(pickle_file_name):
    print('using existing file info')
    with open(pickle_file_name, 'rb') as pf:
        all_input_files = pickle.load(pf)
else:
    print('collecting new file info')
    all_input_files = defaultdict(list)
    for root, dirs, files in os.walk(RAW_PATH):
        for f in files:
            all_input_files[f].append(os.path.join(root, f))
    with open(pickle_file_name, 'wb') as pf:
        pickle.dump(all_input_files, pf)



# process them
with Pool() as p: p.map(process_file, all_input_files.items())
